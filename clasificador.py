# -*- coding: utf-8 -*-
# ===============================================================
# üåæ PREDWEEM v5.1 ‚Äî Mixture-of-Prototypes (DTW + Monotone)
# ===============================================================
# - K prototipos (k-medoids con DTW, sin libs extra)
# - Clasificador:
#       meteo + inicio_emergencia + din√°mica JD30‚Äì120
#       (tasa promedio, incremento m√°x, d√≠a de incremento m√°x, fracci√≥n 1‚Äì120)
#       ‚Üí patr√≥n (GradientBoostingClassifier)
# - Curva predicha = mezcla convexa de prototipos + warp (shift/scale)
# - Monoton√≠a garantizada (acumulado de incrementos ‚â• 0)
# - Clasificaci√≥n de patrones basada SOLO en la curva entre JD 30‚Äì121 (DTW)
# - M√≥dulo para comparar curva real vs predicha (RMSE/MAE)
# - Rango JD 1..274 (1-ene ‚Üí 1-oct)
# ===============================================================

import streamlit as st
import numpy as np
import pandas as pd
import altair as alt
import re, io, joblib
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler

# ---------------------------------------------------------------
# CONFIGURACI√ìN GENERAL STREAMLIT
# ---------------------------------------------------------------
st.set_page_config(page_title="PREDWEEM v5.1 ‚Äî Mixture-of-Prototypes (DTW)", layout="wide")
st.title("üåæ PREDWEEM v5.1 ‚Äî Mixture-of-Prototypes (DTW + Monotone)")

JD_MAX = 274          # Trabajamos hasta el 1 de octubre
XRANGE = (1, JD_MAX)  # Rango del eje X en gr√°ficos

# ===============================================================
# UTILIDADES GENERALES
# ===============================================================
def _make_unique(names):
    """
    Hace √∫nicos los nombres de columna sin usar APIs internas de pandas.
    Si hay columnas repetidas, les agrega .1, .2, etc.
    """
    seen, out = {}, []
    for n in names:
        if n not in seen:
            seen[n] = 0
            out.append(n)
        else:
            seen[n] += 1
            out.append(f"{n}.{seen[n]}")
    return out

def standardize_cols(df: pd.DataFrame) -> pd.DataFrame:
    """
    Normaliza nombres de columnas y trata de mapearlos a:
    - tmin, tmax, prec, jd, fecha
    """
    df = df.copy()
    df.columns = _make_unique([str(c).lower().strip() for c in df.columns])
    ren = {
        "temperatura minima":"tmin","t_min":"tmin","t min":"tmin","m√≠nima":"tmin","min":"tmin",
        "temperatura maxima":"tmax","t_max":"tmax","t max":"tmax","m√°xima":"tmax","max":"tmax",
        "precipitacion":"prec","precip":"prec","pp":"prec","lluvia":"prec","rain":"prec",
        "dia juliano":"jd","d√≠a juliano":"jd","julian_days":"jd","dia":"jd","d√≠a":"jd",
        "fecha":"fecha","date":"fecha"
    }
    for k,v in ren.items():
        if k in df.columns:
            df = df.rename(columns={k:v})
    if "fecha" in df.columns:
        df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce", dayfirst=True)
    for c in ["tmin","tmax","prec","jd"]:
        if c in df.columns and isinstance(df[c], pd.Series):
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

def ensure_jd_1_to_274(df: pd.DataFrame) -> pd.DataFrame:
    """
    Asegura que la tabla tenga una columna 'jd' (d√≠a juliano) y la
    reindexa al rango 1..274 con interpolaci√≥n y relleno.
    """
    df = df.copy()
    df.columns = _make_unique(df.columns)
    if "jd" not in df.columns:
        # Si hay fecha, derivar jd; si no, asumir secuencia 1..n
        if "fecha" in df.columns and df["fecha"].notna().any():
            y0 = int(df["fecha"].dt.year.mode().iloc[0])
            df = df[(df["fecha"] >= f"{y0}-01-01") & (df["fecha"] <= f"{y0}-10-01")].copy().sort_values("fecha")
            df["jd"] = df["fecha"].dt.dayofyear - pd.Timestamp(f"{y0}-01-01").dayofyear + 1
        else:
            df["jd"] = np.arange(1, len(df) + 1, dtype=int)
    if isinstance(df["jd"], pd.DataFrame):
        df["jd"] = df["jd"].iloc[:,0]
    df["jd"] = pd.to_numeric(df["jd"], errors="coerce").astype("Int64")

    jd_range = np.arange(1, JD_MAX+1)
    df = (df.set_index("jd")
            .reindex(jd_range)
            .interpolate()
            .ffill().bfill()
            .reset_index())
    return df

# ===============================================================
# CURVAS DE EMERGENCIA (HIST√ìRICA O REAL) DESDE XLSX
# ===============================================================
def curva_desde_xlsx_anual(file) -> np.ndarray:
    """
    Lee XLSX con dos columnas [d√≠a/fecha, valor] (diaria o semanal) y
    devuelve curva acumulada 0..1 (JD 1..274). Si la serie es semanal,
    suaviza con ventana de 7 d√≠as.
    """
    df = pd.read_excel(file, header=None)
    if df.shape[1] < 2:
        df = pd.read_excel(file)

    col0 = pd.to_numeric(df.iloc[:,0], errors="coerce")
    col1 = pd.to_numeric(df.iloc[:,1], errors="coerce").fillna(0.0)

    if col0.isna().mean() > 0.5:
        # Primera columna es fecha
        fch = pd.to_datetime(df.iloc[:,0], errors="coerce", dayfirst=True)
        jd  = fch.dt.dayofyear
        val = col1
    else:
        jd  = col0.astype("Int64")
        val = col1

    jd_clean = jd.dropna().astype(int).sort_values().unique()
    paso = int(np.median(np.diff(jd_clean))) if len(jd_clean)>1 else 7

    # Arreglo diario a√±o completo (365)
    daily = np.zeros(365, dtype=float)
    for d,v in zip(jd,val):
        if pd.notna(d) and 1 <= int(d) <= 365:
            daily[int(d)-1] += float(v)
    # Si es semanal u otra frecuencia >1, suavizar
    if paso > 1:
        daily = np.convolve(daily, np.ones(7)/7, mode="same")

    # Acumulada y normalizaci√≥n a 0..1
    acum = np.cumsum(daily)
    if np.nanmax(acum) == 0:
        return np.zeros(JD_MAX, dtype=float)
    curva = (acum / np.nanmax(acum))[:JD_MAX]
    # Asegurar monoton√≠a no decreciente
    return np.maximum.accumulate(np.clip(curva,0,1))

def emerg_rel_7d_from_acum(y_acum: np.ndarray) -> np.ndarray:
    """
    Emergencia relativa semanal: deriva la acumulada diaria y aplica
    promedio m√≥vil de 7 d√≠as.
    """
    inc = np.diff(np.insert(y_acum, 0, 0.0))
    return np.convolve(inc, np.ones(7)/7, mode="same")

def frac_curva_1_120(y_acum: np.ndarray) -> float:
    """
    Fracci√≥n de emergencia acumulada al d√≠a juliano 120.
    Dado que la curva est√° normalizada 0‚Äì1, es simplemente E(120).
    (Se usa como factor de clasificaci√≥n y diagn√≥stico).
    """
    if len(y_acum) == 0:
        return 0.0
    idx_120 = min(119, len(y_acum)-1)  # JD120 ‚Üí √≠ndice 119
    return float(y_acum[idx_120])

def detectar_inicio_emergencia(curva: np.ndarray) -> int:
    """
    Detecta el d√≠a juliano de inicio de la emergencia.

    Definici√≥n:
      ‚ûú primer d√≠a (JD) donde la emergencia acumulada es > 0,
         contando desde el d√≠a juliano 1.

    Si nunca supera 0, devuelve 999 (indicador de 'desconocido').
    """
    idx = np.where(curva > 0)[0]
    if len(idx) == 0:
        return 999
    return int(idx[0] + 1)  # √≠ndice 0-based ‚Üí JD (1-based)

def analizar_incrementos_30_120(curva: np.ndarray):
    """
    Analiza el comportamiento de la curva entre JD 30 y JD 120:
      - tasa_promedio_30_120: incremento medio diario
      - max_incremento_30_120: mayor ‚àÜE en un d√≠a
      - dia_max_incremento_30_120: JD donde ocurre ese m√°ximo ‚àÜE

    Se usa como factor de clasificaci√≥n y tambi√©n para diagn√≥stico.
    """
    i1, i2 = 29, 119  # JD30=idx29, JD120=idx119
    segmento = curva[i1:i2+1]

    if len(segmento) < 2:
        return {
            "tasa_promedio_30_120": 0.0,
            "max_incremento_30_120": 0.0,
            "dia_max_incremento_30_120": 30
        }

    # Incrementos diarios en ese segmento
    inc = np.diff(segmento)

    tasa_promedio = (segmento[-1] - segmento[0]) / (i2 - i1)
    max_inc = float(np.max(inc))
    idx_max_inc = int(np.argmax(inc))
    dia_max_inc = 30 + idx_max_inc   # convertir a d√≠a juliano

    return {
        "tasa_promedio_30_120": float(tasa_promedio),
        "max_incremento_30_120": max_inc,
        "dia_max_incremento_30_120": dia_max_inc
    }

# ===============================================================
# FEATURES METEOROL√ìGICAS (robusto)
# ===============================================================
FEATURE_ORDER = [
    "gdd5_FM","gdd3_FM","pp_FM","ev10_FM","ev20_FM",
    "dry_run_FM","wet_run_FM","tmed14_May","tmed28_May","gdd5_120","pp_120"
]

def _longest_run(binary_vec: np.ndarray) -> int:
    """
    Longitud m√°xima de racha consecutiva de 1s (por ejemplo,
    d√≠as secos o h√∫medos consecutivos).
    """
    m = c = 0
    for v in binary_vec:
        c = c + 1 if v == 1 else 0
        m = max(m, c)
    return int(m)

def build_features_meteo(dfm: pd.DataFrame):
    """
    A partir de la serie meteo diaria (tmin, tmax, prec, jd) en 1..274,
    calcula un conjunto de features agroclim√°ticos agregados:
    - GDD base 5 y 3 en Feb‚ÄìMay
    - Precipitaciones totales, eventos ‚â•10 mm, ‚â•20 mm
    - Racha seca y h√∫meda m√°s larga
    - Tmed 14 y 28 d√≠as centrada en mayo
    - GDD5 acumulado al JD 120
    - Precipitaci√≥n acumulada al JD 120
    """
    dfm = standardize_cols(dfm)
    dfm = ensure_jd_1_to_274(dfm)
    tmin = dfm["tmin"].astype(float).to_numpy()
    tmax = dfm["tmax"].astype(float).to_numpy()
    tmed = (tmin + tmax) / 2.0
    prec = dfm["prec"].astype(float).to_numpy()
    jd   = dfm["jd"].astype(int).to_numpy()

    # Ventana Feb‚ÄìMay (aprox JD 32‚Äì151)
    mask_FM = (jd >= 32) & (jd <= 151)
    gdd5 = np.cumsum(np.maximum(tmed - 5, 0))
    gdd3 = np.cumsum(np.maximum(tmed - 3, 0))

    if not np.any(mask_FM):
        # Si algo raro, usar todo el rango
        mask_FM = np.ones_like(jd, dtype=bool)

    pf = prec[mask_FM]
    if pf.size == 0 or np.all(np.isnan(pf)):
        pf = np.zeros(1)

    f = {}
    f["gdd5_FM"]   = float(np.ptp(gdd5[mask_FM])) if np.any(~np.isnan(gdd5[mask_FM])) else 0.0
    f["gdd3_FM"]   = float(np.ptp(gdd3[mask_FM])) if np.any(~np.isnan(gdd3[mask_FM])) else 0.0
    f["pp_FM"]     = float(np.nansum(pf))
    f["ev10_FM"]   = int(np.nansum(pf >= 10))
    f["ev20_FM"]   = int(np.nansum(pf >= 20))
    dry            = np.nan_to_num(pf < 1, nan=0).astype(int)
    wet            = np.nan_to_num(pf >= 5, nan=0).astype(int)
    f["dry_run_FM"]= _longest_run(dry)
    f["wet_run_FM"]= _longest_run(wet)

    # Tmed suavizada alrededor de mayo (14 y 28 d√≠as)
    def ma(x, w):
        k = np.ones(w) / w
        return np.convolve(x, k, "same")
    idx_may = min(150, len(tmed)-1)
    f["tmed14_May"] = float(ma(tmed, 14)[idx_may])
    f["tmed28_May"] = float(ma(tmed, 28)[idx_may])

    # Estado a JD 120
    idx_120 = min(119, len(tmed) - 1)
    f["gdd5_120"] = float(gdd5[idx_120])
    f["pp_120"]   = float(np.nansum(prec[: idx_120 + 1]))

    # Orden consistente
    f = {k: f[k] for k in FEATURE_ORDER}
    return dfm, f

# ===============================================================
# DTW + K-MEDOIDS (SIN DEPENDENCIAS EXTERNAS)
# ===> Importante: usa s√≥lo el tramo JD 30‚Äì121 para comparar curvas
# ===============================================================
def dtw_distance(a: np.ndarray, b: np.ndarray) -> float:
    """
    Distancia DTW entre dos curvas de emergencia acumulada,
    usando √∫nicamente el segmento JD 30‚Äì121 (inclusive).
    Esto asegura que la clasificaci√≥n de patrones se base s√≥lo
    en la parte temprana de la curva.
    """
    # Recortar a ventana 30‚Äì121 (√≠ndices 29..120)
    a_seg = a[29:121]
    b_seg = b[29:121]

    n, m = len(a_seg), len(b_seg)
    D = np.full((n+1, m+1), np.inf, dtype=float)
    D[0,0] = 0.0
    for i in range(1, n+1):
        ai = a_seg[i-1]
        for j in range(1, m+1):
            cost = (ai - b_seg[j-1])**2
            D[i,j] = cost + min(D[i-1,j], D[i,j-1], D[i-1,j-1])
    return float(np.sqrt(D[n,m]))

def k_medoids_dtw(curves: list, K: int, max_iter: int = 50, seed: int = 42):
    """
    Agrupa curvas en K clusters usando k-medoids con distancia DTW
    basada s√≥lo en JD 30‚Äì121. Devuelve:
    - √≠ndices de los medoids (prototipos),
    - asignaci√≥n de miembros a clusters,
    - matriz de distancias DTW.
    """
    rng = np.random.default_rng(seed)
    N = len(curves)
    if K > N:
        K = N
    idx = rng.choice(N, size=K, replace=False)
    medoid_idx = list(idx)

    # Matriz de distancias (sim√©trica)
    D = np.zeros((N,N), float)
    for i in range(N):
        for j in range(i+1, N):
            d = dtw_distance(curves[i], curves[j])
            D[i,j] = D[j,i] = d

    # Iterar hasta convergencia de medoids
    for _ in range(max_iter):
        assign = np.argmin(D[:, medoid_idx], axis=1)
        new_medoids = []
        for k in range(K):
            members = np.where(assign == k)[0]
            if len(members) == 0:
                # Si queda vac√≠o, mantener el medoid actual
                new_medoids.append(medoid_idx[k])
                continue
            subD = D[np.ix_(members, members)]
            sums = subD.sum(axis=1)
            chosen = members[np.argmin(sums)]
            new_medoids.append(chosen)
        if new_medoids == medoid_idx:
            break
        medoid_idx = new_medoids

    clusters = {k: [] for k in range(K)}
    assign = np.argmin(D[:, medoid_idx], axis=1)
    for i in range(N):
        clusters[int(assign[i])].append(i)
    return medoid_idx, clusters, D

# ===============================================================
# BUNDLE HELPERS ‚Äî warp + mezcla convexa
# ===============================================================
def warp_curve(proto: np.ndarray, shift: float, scale: float) -> np.ndarray:
    """
    Aplica un warp simple a la curva prototipo:
    - shift: desplazamiento horizontal en d√≠as
    - scale: escalado del eje temporal (compresi√≥n/estiramiento)
    """
    t = np.arange(1, JD_MAX+1, dtype=float)
    tp = (t - shift) / max(scale, 1e-6)
    tp = np.clip(tp, 1, JD_MAX)
    yv = np.interp(tp, np.arange(1, JD_MAX+1, dtype=float), proto)
    return np.maximum.accumulate(np.clip(yv, 0, 1))

def mezcla_convexa(protos: np.ndarray, proba: np.ndarray, k_hat: int, shift: float, scale: float) -> np.ndarray:
    """
    Construye la curva predicha como mezcla convexa de todos los prototipos,
    aplicando el warp (shift/scale) s√≥lo al patr√≥n m√°s probable.
    """
    K = protos.shape[0]
    mix = np.zeros(JD_MAX, float)
    for k in range(K):
        yk = warp_curve(protos[k], shift if k==k_hat else 0.0,
                        scale if k==k_hat else 1.0)
        mix += float(proba[k]) * yk
    return np.maximum.accumulate(np.clip(mix, 0, 1))

# ===============================================================
# DEFINICI√ìN DE FEATURES PARA REGRESORES Y CLASIFICADOR
# ===============================================================
REG_FEAT_NAMES = FEATURE_ORDER + ["inicio_emergencia"]
DYN_FEAT_NAMES = ["frac_1_120", "tasa_prom_30_120", "max_inc_30_120", "dia_max_inc_30_120"]
CLF_FEAT_NAMES = REG_FEAT_NAMES + DYN_FEAT_NAMES

# ===============================================================
# APP ‚Äî TABS
# ===============================================================
tab1, tab2, tab3 = st.tabs([
    "üß™ Entrenar prototipos + clasificador",
    "üîÆ Identificar patrones y predecir",
    "üìà Comparar Real vs Predicci√≥n"
])

# ---------------------------------------------------------------
# TAB 1 ‚Äî ENTRENAMIENTO
# ---------------------------------------------------------------
with tab1:
    st.subheader("üß™ Entrenamiento (k-medoids DTW + mezcla de prototipos)")
    st.markdown("""
    Sub√≠:
    - **Meteorolog√≠a multianual** (una hoja por a√±o)
    - **Curvas hist√≥ricas de emergencia** (1 archivo XLSX por a√±o)
    - Opcional: **archivo con inicio de emergencia medido a campo** (A√±o, JD_inicio)

    El modelo:
    1. Aprende K prototipos de curva (k-medoids con DTW entre JD 30‚Äì121)
    2. Calcula, para cada curva hist√≥rica:
       - inicio_emergencia
       - fracci√≥n acumulada al JD 120
       - tasa promedio JD30‚Äì120
       - incremento m√°ximo JD30‚Äì120
       - d√≠a del incremento m√°ximo JD30‚Äì120
    3. Ajusta regresores meteo+inicio ‚Üí (estas 4 variables din√°micas)
    4. Construye un clasificador meteo + inicio + din√°mica (JD30‚Äì120) ‚Üí patr√≥n
    5. Ajusta warps (shift/scale) por cluster
    """)

    meteo_book = st.file_uploader("üìò Meteorolog√≠a multianual (una hoja por a√±o)", type=["xlsx","xls"])
    curvas_files = st.file_uploader("üìà Curvas hist√≥ricas (XLSX por a√±o, acumulada o semanal)",
                                    type=["xlsx","xls"], accept_multiple_files=True)

    inicio_file = st.file_uploader(
        "üìç (Opcional) Archivo con inicio de emergencia medido a campo por a√±o (A√±o, JD_inicio)",
        type=["csv", "xlsx"]
    )

    K = st.slider("N√∫mero de prototipos/patrones (K)", 2, 10, 10, 1)
    seed = st.number_input("Semilla", 0, 99999, 42)
    btn_train = st.button("üöÄ Entrenar")

    if btn_train:
        if not (meteo_book and curvas_files):
            st.error("Carg√° meteorolog√≠a y curvas.")
            st.stop()

        # 0) Si hay archivo de inicio medido a campo, cargarlo como dict a√±o‚ÜíJD
        inicio_medido = {}
        if inicio_file is not None:
            if inicio_file.name.lower().endswith(".csv"):
                df_inicio = pd.read_csv(inicio_file)
            else:
                df_inicio = pd.read_excel(inicio_file)
            df_inicio.columns = [str(c).strip().lower() for c in df_inicio.columns]
            col_anio = None
            col_jd   = None
            for c in df_inicio.columns:
                if "a√±o" in c or "ano" in c or "year" in c:
                    col_anio = c
                if "inicio" in c or "jd" in c:
                    col_jd = c
            if col_anio and col_jd:
                for _, row in df_inicio.iterrows():
                    try:
                        y = int(row[col_anio])
                        jd_ini = int(row[col_jd])
                        inicio_medido[y] = jd_ini
                    except:
                        continue
                st.info(f"üìç Inicio de emergencia medido cargado para a√±os: {sorted(inicio_medido.keys())}")
            else:
                st.warning("No se encontraron columnas claras de a√±o / inicio en el archivo de inicio de emergencia.")

        # 1) Leer meteo por a√±o
        sheets = pd.read_excel(meteo_book, sheet_name=None)
        meteo_dict = {}
        for name, df in sheets.items():
            df = standardize_cols(df)
            df = ensure_jd_1_to_274(df)
            try:
                year = int(re.findall(r"\d{4}", str(name))[0])
            except:
                year = None
            if year and all(c in df.columns for c in ["tmin","tmax","prec"]):
                meteo_dict[year] = df[["jd","tmin","tmax","prec"]].copy()

        if not meteo_dict:
            st.error("‚õî No se detect√≥ meteorolog√≠a v√°lida por a√±o.")
            st.stop()
        st.success(f"‚úÖ Meteorolog√≠a v√°lida: {len(meteo_dict)} a√±os")

        # 2) Leer curvas por a√±o
        years_list, curves_list = [], []
        curves_dict = {}   # para acceso por a√±o
        for f in curvas_files:
            y4 = re.findall(r"(\d{4})", f.name)
            year = int(y4[0]) if y4 else None
            if year is None:
                continue
            curva = np.maximum.accumulate(curva_desde_xlsx_anual(f))
            if curva.max() > 0:
                curva = curva[:JD_MAX]
                curves_list.append(curva)
                years_list.append(year)
                curves_dict[year] = curva
        if not years_list:
            st.error("‚õî No se detectaron curvas v√°lidas.")
            st.stop()

        # 3) Intersecci√≥n meteo‚Äìcurvas
        common_years = sorted([y for y in years_list if y in meteo_dict])
        if len(common_years) < 3:
            st.error("‚õî Muy pocos a√±os en com√∫n (se recomienda ‚â• 5).")
            st.stop()
        curves = [curves_dict[y] for y in common_years]

        # 4) Detectar inicio de emergencia y din√°mica 30‚Äì120 por a√±o
        inicio_year = {}
        Z_rows = []          # features para regresores din√°micos
        y_frac = []
        y_tasa = []
        y_max  = []
        y_dia  = []
        X_clf_rows = []      # features para clasificador / warps

        for y_idx, y in enumerate(common_years):
            curva = curves_dict[y]

            # inicio_emergencia
            if y in inicio_medido:
                ini = int(inicio_medido[y])
            else:
                ini = detectar_inicio_emergencia(curva)
            inicio_year[y] = ini

            # din√°mica JD30‚Äì120
            anal = analizar_incrementos_30_120(curva)
            frac120 = frac_curva_1_120(curva)

            # features meteo
            _, f_meteo = build_features_meteo(meteo_dict[y])

            # ----- features para regresores din√°micos -----
            z_dict = f_meteo.copy()
            z_dict["inicio_emergencia"] = ini
            z_row = [z_dict[k] for k in REG_FEAT_NAMES]
            Z_rows.append(z_row)
            y_frac.append(frac120)
            y_tasa.append(anal["tasa_promedio_30_120"])
            y_max.append(anal["max_incremento_30_120"])
            y_dia.append(anal["dia_max_incremento_30_120"])

            # ----- features para clasificador (meteo+inicio+din√°mica) -----
            clf_dict = z_dict.copy()
            clf_dict["frac_1_120"] = frac120
            clf_dict["tasa_prom_30_120"] = anal["tasa_promedio_30_120"]
            clf_dict["max_inc_30_120"] = anal["max_incremento_30_120"]
            clf_dict["dia_max_inc_30_120"] = anal["dia_max_incremento_30_120"]
            X_clf_rows.append([clf_dict[k] for k in CLF_FEAT_NAMES])

        st.write("üìç D√≠a de inicio de emergencia por a√±o (medido o detectado):", inicio_year)

        # 5) k-medoids (DTW sobre JD 30‚Äì121)
        st.info("üßÆ Calculando k-medoids (DTW, JD 30‚Äì121)...")
        medoid_idx, clusters, D = k_medoids_dtw(curves, K=K, max_iter=50, seed=seed)
        protos = [curves[i] for i in medoid_idx]

        # 6) Etiquetas de cluster por a√±o
        assign = np.argmin(D[:, np.array(medoid_idx)], axis=1)  # √≠ndice cluster 0..K-1
        y_lbl = assign.astype(int)

        # A√±os por cluster para interpretaci√≥n
        cluster_years = {k: [] for k in range(K)}
        for i, y in enumerate(common_years):
            cluster_years[int(y_lbl[i])].append(int(y))

        # 7) Entrenar regresores din√°micos meteo+inicio ‚Üí din√°mica curva 30‚Äì120
        Z = np.array(Z_rows, float)
        y_frac = np.array(y_frac, float)
        y_tasa = np.array(y_tasa, float)
        y_max  = np.array(y_max, float)
        y_dia  = np.array(y_dia, float)

        reg_frac = GradientBoostingRegressor(random_state=seed)
        reg_tasa = GradientBoostingRegressor(random_state=seed)
        reg_max  = GradientBoostingRegressor(random_state=seed)
        reg_dia  = GradientBoostingRegressor(random_state=seed)

        reg_frac.fit(Z, y_frac)
        reg_tasa.fit(Z, y_tasa)
        reg_max.fit(Z, y_max)
        reg_dia.fit(Z, y_dia)

        # 8) Entrenar clasificador meteo+inicio+din√°mica ‚Üí patr√≥n
        X_clf_raw = np.array(X_clf_rows, float)
        xsc_clf = StandardScaler().fit(X_clf_raw)
        Xs_clf = xsc_clf.transform(X_clf_raw)

        clf = GradientBoostingClassifier(random_state=seed)
        clf.fit(Xs_clf, y_lbl)

        # 9) Warps (shift/scale) por cluster usando Xs_clf
        regs_shift, regs_scale = {}, {}
        for k in range(K):
            idx_k = np.where(y_lbl == k)[0]
            if len(idx_k) == 0:
                continue
            proto = protos[k]
            shifts, scales, Xk = [], [], []
            for ii in idx_k:
                curv = curves[ii]
                best = (0.0, 1.0, 1e9)
                # B√∫squeda gruesa de shift y scale
                for sh in range(-20, 21, 5):       # ¬±20 d√≠as
                    for sc in [0.9, 0.95, 1.0, 1.05, 1.1]:
                        cand = warp_curve(proto, sh, sc)
                        rmse = float(np.sqrt(np.mean((cand - curv)**2)))
                        if rmse < best[2]:
                            best = (float(sh), float(sc), rmse)
                shifts.append(best[0])
                scales.append(best[1])
                Xk.append(Xs_clf[ii])
            Xk = np.vstack(Xk)
            regs_shift[k] = GradientBoostingRegressor(random_state=seed).fit(Xk, np.array(shifts))
            regs_scale[k] = GradientBoostingRegressor(random_state=seed).fit(Xk, np.array(scales))

        # 10) Guardar bundle
        bundle = {
            "xsc_clf": xsc_clf,
            "feat_names_reg": REG_FEAT_NAMES,    # para regresores de din√°mica
            "feat_names_clf": CLF_FEAT_NAMES,    # para clasificador y warps
            "clf": clf,
            "protos": np.vstack(protos),         # K x 274
            "regs_shift": regs_shift,
            "regs_scale": regs_scale,
            "cluster_years": cluster_years,
            "reg_frac": reg_frac,
            "reg_tasa": reg_tasa,
            "reg_max": reg_max,
            "reg_dia": reg_dia
        }
        st.success(f"‚úÖ Entrenamiento OK. K={K} prototipos.")
        st.session_state["mix_bundle"] = bundle

        buf = io.BytesIO()
        joblib.dump(bundle, buf)
        st.download_button(
            "üíæ Descargar modelo (joblib)",
            data=buf.getvalue(),
            file_name=f"predweem_v51_mixture_dtw_K{K}.joblib",
            mime="application/octet-stream"
        )

        # 11) Vista r√°pida de prototipos
        dias = np.arange(1, JD_MAX+1)
        dfp = []
        for k, proto in enumerate(protos):
            years_txt = ", ".join(map(str, cluster_years.get(k, []))) if cluster_years.get(k) else "‚Äî"
            dfp.append(pd.DataFrame({
                "D√≠a": dias,
                "Valor": proto,
                "Serie": f"Proto {k} ¬∑ a√±os: {years_txt}"
            }))
        dfp = pd.concat(dfp)
        chart = alt.Chart(dfp).mark_line().encode(
            x=alt.X("D√≠a:Q", scale=alt.Scale(domain=list(XRANGE))),
            y=alt.Y("Valor:Q", title="Emergencia acumulada (0‚Äì1)", scale=alt.Scale(domain=[0,1])),
            color="Serie:N"
        ).properties(
            height=420,
            title="Prototipos (medoids DTW, clasificaci√≥n basada en JD 30‚Äì121)"
        )
        st.altair_chart(chart, use_container_width=True)

# ---------------------------------------------------------------
# TAB 2 ‚Äî PREDICCI√ìN
# ---------------------------------------------------------------
with tab2:
    st.subheader("üîÆ Identificaci√≥n de patrones y predicci√≥n a partir de meteorolog√≠a nueva")

    st.markdown("""
    Carg√°:
    - Un **modelo entrenado** (.joblib)  
    - La **meteorolog√≠a diaria** del a√±o que quer√©s analizar  

    Opcionalmente pod√©s ingresar el **d√≠a de inicio de emergencia medido a campo**.
    Si lo dej√°s en 0, el modelo usar√° 999 como valor 'desconocido' para esa variable.
    """)

    modelo_file = st.file_uploader("üì¶ Modelo (predweem_v51_mixture_dtw_*.joblib)", type=["joblib"])
    meteo_file  = st.file_uploader("üìò Meteorolog√≠a nueva (XLSX)", type=["xlsx","xls"])
    inicio_manual = st.number_input(
        "üìç D√≠a de inicio de emergencia medido a campo (0 = desconocido)",
        min_value=0, max_value=JD_MAX, value=0, step=1
    )

    btn_pred = st.button("üöÄ Analizar y predecir")

    if btn_pred:
        if not (modelo_file and meteo_file):
            st.error("Carg√° el modelo y la meteo.")
            st.stop()

        # --- Cargar modelo ---
        bundle = joblib.load(modelo_file)
        xsc_clf = bundle["xsc_clf"]
        feat_names_reg = bundle["feat_names_reg"]
        feat_names_clf = bundle["feat_names_clf"]
        clf = bundle["clf"]
        protos = bundle["protos"]
        regs_shift = bundle["regs_shift"]
        regs_scale = bundle["regs_scale"]
        cluster_years = bundle.get("cluster_years", {})
        reg_frac = bundle["reg_frac"]
        reg_tasa = bundle["reg_tasa"]
        reg_max  = bundle["reg_max"]
        reg_dia  = bundle["reg_dia"]
        K = protos.shape[0]

        # --- Features desde meteo nueva ---
        dfm = pd.read_excel(meteo_file)
        dfm, f_new = build_features_meteo(dfm)

        # Construimos diccionario de features para regresores (meteo + inicio_emergencia)
        reg_dict = f_new.copy()
        if inicio_manual > 0:
            reg_dict["inicio_emergencia"] = float(inicio_manual)
        else:
            reg_dict["inicio_emergencia"] = 999.0  # valor 'desconocido' / neutro

        z_row = [reg_dict[k] for k in feat_names_reg]
        Z_pred = np.array([z_row], float)

        # --- Estimar din√°mica JD30‚Äì120 para este a√±o ---
        frac_est = float(reg_frac.predict(Z_pred)[0])
        tasa_est = float(reg_tasa.predict(Z_pred)[0])
        max_est  = float(reg_max.predict(Z_pred)[0])
        dia_est  = float(reg_dia.predict(Z_pred)[0])

        # --- Vector de features para clasificador (meteo+inicio+din√°mica estimada) ---
        clf_dict = reg_dict.copy()
        clf_dict["frac_1_120"] = frac_est
        clf_dict["tasa_prom_30_120"] = tasa_est
        clf_dict["max_inc_30_120"] = max_est
        clf_dict["dia_max_inc_30_120"] = dia_est

        xrow_clf = [clf_dict[k] for k in feat_names_clf]
        Xs = xsc_clf.transform([np.array(xrow_clf, float)])

        # --- Probabilidades de cada patr√≥n ---
        proba  = clf.predict_proba(Xs)[0]  # shape (K,)
        top_idx = np.argsort(proba)[::-1]
        k_hat = int(top_idx[0])

        # --- Warp predicho para el patr√≥n m√°s probable ---
        if k_hat in regs_shift:
            shift = float(regs_shift[k_hat].predict(Xs)[0])
        else:
            shift = 0.0
        if k_hat in regs_scale:
            scale = float(regs_scale[k_hat].predict(Xs)[0])
        else:
            scale = 1.0
        scale = float(np.clip(scale, 0.9, 1.1))

        # --- Curva predicha (mezcla convexa) y patr√≥n m√°s probable ---
        mix = mezcla_convexa(protos, proba, k_hat, shift, scale)
        proto_hat = protos[k_hat]

        # --- Emergencia relativa semanal (sobre la predicci√≥n) ---
        rel7 = emerg_rel_7d_from_acum(mix)

        # --- Fracci√≥n de la curva entre JD 1‚Äì120 (predicha desde la curva) ---
        frac120_pred_curva = frac_curva_1_120(mix)

        # --- An√°lisis din√°mico de la curva predicha (usando la curva resultante) ---
        analisis_pred = analizar_incrementos_30_120(mix)

        st.markdown(f"""
### üîç Din√°mica estimada usada para la clasificaci√≥n (a priori, desde meteo + inicio)
- **Fracci√≥n 1‚Äì120 (estimada):** `{frac_est:.4f}`
- **Tasa promedio 30‚Äì120 (estimada):** `{tasa_est:.4f}` por d√≠a  
- **Incremento m√°ximo 30‚Äì120 (estimado):** `{max_est:.4f}`  
- **D√≠a de incremento m√°ximo (estimado):** `JD {dia_est:.1f}`  

### üîç Din√°mica ex-post de la curva predicha (a partir de la curva acumulada final)
- **Fracci√≥n 1‚Äì120 (calculada):** `{frac120_pred_curva:.4f}`
- **Tasa promedio 30‚Äì120 (calculada):** `{analisis_pred['tasa_promedio_30_120']:.4f}`  
- **Incremento m√°ximo 30‚Äì120 (calculado):** `{analisis_pred['max_incremento_30_120']:.4f}`  
- **D√≠a de incremento m√°ximo (calculado):** `JD {analisis_pred['dia_max_incremento_30_120']}`  
""")

        # --- Gr√°fico: Predicci√≥n + Patr√≥n m√°s probable + Relativa 7d ---
        dias = np.arange(1, JD_MAX + 1)
        df_plot = pd.DataFrame({
            "D√≠a": dias,
            "Predicci√≥n": mix,
            "Patr√≥n m√°s probable": proto_hat,
            "Emergencia_relativa_7d": rel7
        })

        base = alt.Chart(df_plot).encode(
            x=alt.X("D√≠a:Q", scale=alt.Scale(domain=list(XRANGE)))
        )

        curva_lineas = base.transform_fold(
            ["Predicci√≥n", "Patr√≥n m√°s probable"], as_=["Serie", "Valor"]
        ).mark_line(strokeWidth=2).encode(
            y=alt.Y("Valor:Q", title="Emergencia acumulada (0‚Äì1)",
                    scale=alt.Scale(domain=[0, 1])),
            color=alt.Color("Serie:N", scale=alt.Scale(scheme="tableau10")),
            tooltip=["Serie:N", alt.Tooltip("Valor:Q", format=".3f"), "D√≠a:Q"]
        )

        max_rel = float(np.nanmax(rel7)) if np.isfinite(np.nanmax(rel7)) else 1.0
        barra_rel = base.mark_area(opacity=0.35).encode(
            y=alt.Y("Emergencia_relativa_7d:Q",
                    axis=alt.Axis(title="Emergencia relativa semanal", titleColor="#666"),
                    scale=alt.Scale(domain=[0, max_rel * 1.1]))
        )

        chart = alt.layer(curva_lineas, barra_rel).resolve_scale(y='independent').properties(
            height=420,
            title=(
                f"Predicci√≥n (C{k_hat} ‚Ä¢ conf {proba[k_hat]:.2f} ‚Ä¢ "
                f"shift {shift:+.1f}d ‚Ä¢ scale {scale:.3f} ‚Ä¢ inicio_emergencia={reg_dict['inicio_emergencia']:.0f})"
            )
        )
        st.altair_chart(chart, use_container_width=True)

        # --- Tabla de probabilidades por patr√≥n (a√±os del cluster) ---
        rows = []
        for k in range(K):
            years_txt = ", ".join(map(str, cluster_years.get(k, []))) if cluster_years.get(k) else "‚Äî"
            rows.append((f"C{k}", float(proba[k]), years_txt))
        df_proba = pd.DataFrame(rows, columns=["Cluster","Probabilidad","A√±os (cluster)"]) \
                    .sort_values("Probabilidad", ascending=False).reset_index(drop=True)
        st.markdown("### üî¢ Probabilidades por patr√≥n")
        st.dataframe(df_proba.style.format({"Probabilidad": "{:.3f}"}), use_container_width=True)

        # --- Descarga predicci√≥n (incluye patr√≥n m√°s probable, relativa 7d y din√°mica) ---
        out = pd.DataFrame({
            "D√≠a": dias,
            "Emergencia_predicha": mix,
            "Patr√≥n_mas_probable": proto_hat,
            "Emergencia_relativa_7d": rel7
        })
        # Guardamos en cada fila los par√°metros usados / estimados
        out["Frac_1_120_estimada"] = frac_est
        out["Frac_1_120_curva"] = frac120_pred_curva
        out["tasa_prom_30_120_estimada"] = tasa_est
        out["max_inc_30_120_estimada"] = max_est
        out["dia_max_inc_30_120_estimada"] = dia_est
        out["tasa_prom_30_120_curva"] = analisis_pred["tasa_promedio_30_120"]
        out["max_inc_30_120_curva"] = analisis_pred["max_incremento_30_120"]
        out["dia_max_inc_30_120_curva"] = analisis_pred["dia_max_incremento_30_120"]
        out["inicio_emergencia_usado"] = reg_dict["inicio_emergencia"]

        st.download_button(
            "‚¨áÔ∏è Descargar curvas (CSV)",
            out.to_csv(index=False).encode("utf-8"),
            file_name="curva_predicha_vs_patron.csv",
            mime="text/csv"
        )

# ---------------------------------------------------------------
# TAB 3 ‚Äî COMPARAR CURVA REAL VS PREDICHA (RMSE/MAE)
# ---------------------------------------------------------------
with tab3:
    st.subheader("üìà Comparar curva real vs curva predicha (RMSE/MAE)")

    st.markdown("""
    Carg√°:
    - Un **modelo entrenado** (.joblib)  
    - La **meteorolog√≠a del a√±o** que quer√©s evaluar  
    - La **curva real de emergencia** de ese mismo a√±o (XLSX, diaria o semanal)

    El sistema:
    1. Calcula **inicio_emergencia real** a partir de la curva
    2. Usa meteo + inicio_emergencia real ‚Üí estima din√°mica JD30‚Äì120 (regresores)
    3. Usa meteo + inicio + din√°mica estimada ‚Üí patr√≥n (clasificador)
    4. Construye la curva predicha (mezcla de prototipos + warp)
    5. Calcula **RMSE/MAE**
    6. Compara fracci√≥n y din√°mica JD30‚Äì120 entre real y predicha
    """)

    modelo_cmp = st.file_uploader("üì¶ Modelo", type=["joblib"], key="cmp_model")
    meteo_cmp  = st.file_uploader("üìò Meteorolog√≠a del a√±o", type=["xlsx","xls"], key="cmp_meteo")
    curva_real_file = st.file_uploader("üìà Curva real (XLSX)", type=["xlsx","xls"], key="cmp_curva")

    btn_cmp = st.button("üöÄ Comparar")

    if btn_cmp:
        if not (modelo_cmp and meteo_cmp and curva_real_file):
            st.error("Falta cargar modelo, meteorolog√≠a o curva real.")
            st.stop()

        # --- Cargar modelo ---
        bundle = joblib.load(modelo_cmp)
        xsc_clf = bundle["xsc_clf"]
        feat_names_reg = bundle["feat_names_reg"]
        feat_names_clf = bundle["feat_names_clf"]
        clf = bundle["clf"]
        protos = bundle["protos"]
        regs_shift = bundle["regs_shift"]
        regs_scale = bundle["regs_scale"]
        cluster_years = bundle.get("cluster_years", {})
        reg_frac = bundle["reg_frac"]
        reg_tasa = bundle["reg_tasa"]
        reg_max  = bundle["reg_max"]
        reg_dia  = bundle["reg_dia"]
        K = protos.shape[0]

        # --- Cargar curva real ---
        curva_real = np.maximum.accumulate(curva_desde_xlsx_anual(curva_real_file))[:JD_MAX]
        rel7_real = emerg_rel_7d_from_acum(curva_real)
        frac120_real = frac_curva_1_120(curva_real)
        inicio_real = detectar_inicio_emergencia(curva_real)
        anal_real = analizar_incrementos_30_120(curva_real)

        # --- Cargar y procesar meteo ---
        dfm = pd.read_excel(meteo_cmp)
        dfm, f_new = build_features_meteo(dfm)

        # Features para regresores din√°micos
        reg_dict = f_new.copy()
        reg_dict["inicio_emergencia"] = float(inicio_real)
        z_row = [reg_dict[k] for k in feat_names_reg]
        Z_cmp = np.array([z_row], float)

        # Din√°mica estimada a partir de meteo + inicio_real
        frac_est = float(reg_frac.predict(Z_cmp)[0])
        tasa_est = float(reg_tasa.predict(Z_cmp)[0])
        max_est  = float(reg_max.predict(Z_cmp)[0])
        dia_est  = float(reg_dia.predict(Z_cmp)[0])

        # Features para clasificador
        clf_dict = reg_dict.copy()
        clf_dict["frac_1_120"] = frac_est
        clf_dict["tasa_prom_30_120"] = tasa_est
        clf_dict["max_inc_30_120"] = max_est
        clf_dict["dia_max_inc_30_120"] = dia_est
        xrow_clf = [clf_dict[k] for k in feat_names_clf]
        Xs = xsc_clf.transform([np.array(xrow_clf, float)])

        # --- Clasificaci√≥n ---
        proba = clf.predict_proba(Xs)[0]
        k_hat = int(np.argmax(proba))

        # --- Warps ---
        if k_hat in regs_shift:
            shift = float(regs_shift[k_hat].predict(Xs)[0])
        else:
            shift = 0.0
        if k_hat in regs_scale:
            scale = float(regs_scale[k_hat].predict(Xs)[0])
        else:
            scale = 1.0
        scale = float(np.clip(scale, 0.9, 1.1))

        # --- Curva predicha ---
        curva_pred = mezcla_convexa(protos, proba, k_hat, shift, scale)
        rel7_pred = emerg_rel_7d_from_acum(curva_pred)
        frac120_pred = frac_curva_1_120(curva_pred)
        anal_pred = analizar_incrementos_30_120(curva_pred)

        # --- RMSE & MAE ---
        rmse = float(np.sqrt(np.mean((curva_real - curva_pred)**2)))
        mae  = float(np.mean(np.abs(curva_real - curva_pred)))

        st.success(f"‚úÖ RMSE = {rmse:.4f} ‚Äî MAE = {mae:.4f}")
        st.markdown(
            f"- **Fracci√≥n real al JD 120:** `{frac120_real:.3f}`\n\n"
            f"- **Fracci√≥n predicha al JD 120:** `{frac120_pred:.3f}`\n\n"
            f"- **inicio_emergencia real usado:** JD `{inicio_real}`"
        )

        st.markdown(f"""
### üîç An√°lisis comparativo JD 30‚Äì120

#### üëâ Curva real
- **Tasa promedio:** `{anal_real['tasa_promedio_30_120']:.4f}`
- **Incremento m√°ximo:** `{anal_real['max_incremento_30_120']:.4f}`
- **D√≠a m√°x incremento:** `JD {anal_real['dia_max_incremento_30_120']}`

#### üëâ Curva predicha
- **Tasa promedio:** `{anal_pred['tasa_promedio_30_120']:.4f}`
- **Incremento m√°ximo:** `{anal_pred['max_incremento_30_120']:.4f}`
- **D√≠a m√°x incremento:** `JD {anal_pred['dia_max_incremento_30_120']}`

#### üëâ Din√°mica estimada usada por el clasificador (desde meteo + inicio)
- **Fracci√≥n 1‚Äì120 (estimada):** `{frac_est:.4f}`
- **Tasa promedio 30‚Äì120 (estimada):** `{tasa_est:.4f}`
- **Incremento m√°ximo 30‚Äì120 (estimado):** `{max_est:.4f}`
- **D√≠a incremento m√°ximo (estimado):** `JD {dia_est:.1f}`
""")

        # --- Gr√°fico comparativo ---
        dias = np.arange(1, JD_MAX+1)
        df_cmp = pd.DataFrame({
            "D√≠a": dias,
            "Real": curva_real,
            "Predicci√≥n": curva_pred,
            "Relativa real 7d": rel7_real,
            "Relativa pred 7d": rel7_pred
        })

        base = alt.Chart(df_cmp).encode(
            x=alt.X("D√≠a:Q", scale=alt.Scale(domain=[1, JD_MAX]))
        )

        lineas = base.transform_fold(
            ["Real", "Predicci√≥n"], as_=["Serie", "Valor"]
        ).mark_line(strokeWidth=2).encode(
            y=alt.Y("Valor:Q", title="Emergencia acumulada (0‚Äì1)",
                    scale=alt.Scale(domain=[0,1])),
            color="Serie:N"
        )

        max_rel = max(float(rel7_real.max()), float(rel7_pred.max()))
        areas = base.transform_fold(
            ["Relativa real 7d", "Relativa pred 7d"],
            as_=["Serie", "Valor"]
        ).mark_area(opacity=0.35).encode(
            y=alt.Y("Valor:Q",
                    axis=alt.Axis(title="Emergencia relativa semanal"),
                    scale=alt.Scale(domain=[0, max_rel*1.1])),
            color="Serie:N"
        )

        chart = alt.layer(lineas, areas).resolve_scale(y='independent').properties(
            height=420,
            title=(
                f"Comparaci√≥n Real vs Predicci√≥n (C{k_hat} ‚Ä¢ conf {proba[k_hat]:.2f} ‚Ä¢ "
                f"shift {shift:+.1f}d ‚Ä¢ scale {scale:.3f})"
            )
        )
        st.altair_chart(chart, use_container_width=True)

        # --- Exportar ---
        out = df_cmp.copy()
        out["Error_abs"] = np.abs(curva_real - curva_pred)
        out["Frac_1_120_real"] = frac120_real
        out["Frac_1_120_pred"] = frac120_pred
        out["inicio_emergencia_real"] = inicio_real
        out["RMSE_global"] = rmse
        out["MAE_global"] = mae

        # din√°micas reales vs predichas (curva) y estimadas (regresor)
        out["tasa_prom_30_120_real"] = anal_real["tasa_promedio_30_120"]
        out["max_inc_30_120_real"] = anal_real["max_incremento_30_120"]
        out["dia_max_inc_30_120_real"] = anal_real["dia_max_incremento_30_120"]

        out["tasa_prom_30_120_curva_pred"] = anal_pred["tasa_promedio_30_120"]
        out["max_inc_30_120_curva_pred"] = anal_pred["max_incremento_30_120"]
        out["dia_max_inc_30_120_curva_pred"] = anal_pred["dia_max_incremento_30_120"]

        out["tasa_prom_30_120_estimada"] = tasa_est
        out["max_inc_30_120_estimada"] = max_est
        out["dia_max_inc_30_120_estimada"] = dia_est
        out["Frac_1_120_estimada"] = frac_est

        st.download_button(
            "‚¨áÔ∏è Descargar comparaci√≥n (CSV)",
            out.to_csv(index=False).encode("utf-8"),
            file_name="comparacion_real_vs_pred.csv",
            mime="text/csv"
        )



