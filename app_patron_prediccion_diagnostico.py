import streamlit as st
import pandas as pd
import numpy as np
import altair as alt

st.title("An√°lisis de series de emergencia relativa")

st.markdown("""
Suba uno o varios archivos Excel, cada uno con una serie de emergencia relativa (d√≠a juliano en la primera columna y valor en la segunda). 
La aplicaci√≥n detectar√° autom√°ticamente el tipo de serie (diaria relativa vs. semanal/acumulada), 
generar√° la curva acumulada normalizada, calcular√° m√©tricas (AUC total, AUC hasta d√≠a 121, proporci√≥n al d√≠a 121) 
y clasificar√° cada serie como **CONCENTRADO** o **DISPERSO** seg√∫n el umbral seleccionado.
""")

# Paso 1: Carga de archivos Excel (m√∫ltiples)
archivos = st.file_uploader("Cargar archivos Excel", type=["xlsx"], accept_multiple_files=True)
umbral = st.slider("Umbral para clasificaci√≥n (CONCENTRADO vs DISPERSO)", min_value=0.0, max_value=1.0, value=0.5, step=0.01)

if archivos:
    resultados = []  # lista para guardar m√©tricas de cada archivo
    curvas_data = []  # para datos de curvas para graficar (formato largo)
    
    for archivo in archivos:
        nombre = archivo.name
        # Leer el Excel. Asumimos dos columnas: dia juliano y valor
        try:
            df = pd.read_excel(archivo, header=None)
        except Exception as e:
            st.error(f"‚ùå No se pudo leer {nombre}: {e}")
            continue
        
        # Asegurar que tenemos dos columnas
        if df.shape[1] < 2:
            st.warning(f"El archivo {nombre} no tiene dos columnas, se ignora.")
            continue
        
        # Renombrar columnas para claridad
        df = df.iloc[:, :2]  # tomar solo las dos primeras columnas si hubiera extras
        df.columns = ["dia", "valor"]
        
        # Paso 2: Determinar tipo de serie
        suma_valores = df["valor"].sum(skipna=True)
        es_diaria = False
        # criterio: nombre del archivo contiene '2023', '2024' o '2025' OR suma ~ 1
        if any(str(year) in nombre for year in [2023, 2024, 2025]):
            es_diaria = True
        if suma_valores >= 0.95 and suma_valores <= 1.05:
            # La suma est√° aproximadamente en 1 (¬±5%)
            es_diaria = True
        
        if es_diaria:
            # Serie diaria relativa: aplicar cumsum para obtener acumulada
            df_sorted = df.sort_values("dia")
            df_sorted["acumulado"] = df_sorted["valor"].cumsum()
        else:
            # No es diaria relativa (serie semanal o valores absolutos)
            # Verificar si la serie ya es acumulada (mon√≥tona no decreciente)
            df_sorted = df.sort_values("dia")
            valores = df_sorted["valor"].values
            # Chequear monotonicidad (permitiendo tolerancia peque√±a para flotantes)
            diffs = np.diff(valores)
            if np.all(diffs >= -1e-9):  # si todas las diferencias son >= 0 (mon√≥tono creciente)
                # Ya es acumulada
                df_sorted["acumulado"] = df_sorted["valor"]
            else:
                # Es semanal/diaria absoluta: aplicar cumsum para obtener acumulado
                df_sorted["acumulado"] = df_sorted["valor"].cumsum()
        
        # Paso 3: Interpolar a diario (si la secuencia de d√≠as tiene huecos)
        # Asegurar que el d√≠a 1 est√° presente; si falta, agregar d√≠a 1 con acumulado 0
        if df_sorted["dia"].iloc[0] > 1:
            df_sorted = pd.concat([
                pd.DataFrame({"dia": [1], "acumulado": [0.0]}),
                df_sorted
            ], ignore_index=True)
        # Crear √≠ndice completo de d√≠as hasta el √∫ltimo d√≠a o 365 (lo que sea mayor)
        ultimo_dia = int(df_sorted["dia"].max())
        # Consideramos hasta d√≠a 365 por seguridad (a√±o completo)
        if ultimo_dia < 365:
            ultimo_dia = 365
        # Reindexar la serie acumulada con todos los d√≠as hasta ultimo_dia
        df_indexed = df_sorted.set_index("dia")["acumulado"]
        df_indexed = df_indexed.reindex(range(1, ultimo_dia+1))
        # Interpolar valores faltantes linealmente entre datos existentes
        df_interpolated = df_indexed.interpolate(method='linear')
        # Rellenar cualquier valor posterior al √∫ltimo dato conocido con el √∫ltimo valor (ffill)
        df_interpolated = df_interpolated.ffill()
        # Rellenar valores antes del primer dato conocido (si aplica) con 0 (bfill)
        df_interpolated = df_interpolated.bfill().fillna(0)
        
        # Paso 4: Normalizar al rango [0,1]
        max_val = df_interpolated.iloc[-1]
        if max_val == 0:
            # Si la serie est√° toda en cero (caso extremo sin eventos), saltamos
            norm_series = df_interpolated
        else:
            norm_series = df_interpolated / max_val
        
        # Asegurarse que la serie llega hasta d√≠a 365 (inclusive)
        if norm_series.index.max() < 365:
            # Extender hasta 365 con el √∫ltimo valor (que ser√≠a 1 si max_val>0)
            last_val = norm_series.iloc[-1]
            norm_series = norm_series.reindex(range(1, 366), fill_value=last_val)
        
        # Paso 5: Calcular AUC total, AUC hasta d√≠a 121, proporci√≥n d√≠a 121
        y_values = norm_series.values  # valores de la curva normalizada
        x_days = norm_series.index.values
        # Area total bajo la curva usando m√©todo trapezoidal
        auc_total = float(np.trapz(y_values, x_days))
        # Si el d√≠a 121 excede los datos, asegurarse de no pasarse
        max_day = norm_series.index.max()
        day121 = 121 if 121 <= max_day else max_day
        auc_121 = float(np.trapz(norm_series.loc[1:day121].values, norm_series.loc[1:day121].index.values))
        # Valor acumulado al d√≠a 121
        # Si la serie no llega al 121 (muy improbable), tomar √∫ltimo
        if day121 in norm_series.index:
            prop_121 = float(norm_series.loc[day121])
        else:
            prop_121 = float(norm_series.iloc[-1])
        # Clasificaci√≥n seg√∫n umbral
        clasificacion = "CONCENTRADO" if prop_121 >= umbral else "DISPERSO"
        
        # Guardar resultados para la tabla
        resultados.append({
            "Archivo": nombre,
            "AUC_total": auc_total,
            "AUC_dia121": auc_121,
            "Prop_121": prop_121,
            "Clasificaci√≥n": clasificacion
        })
        
        # Preparar datos para graficar (acumulada normalizada)
        # Usamos nombre del archivo sin extensi√≥n como etiqueta de serie
        serie_label = nombre.rsplit('.', 1)[0]
        df_plot = pd.DataFrame({
            "dia": norm_series.index.values,
            "acumulado_norm": norm_series.values,
            "serie": serie_label
        })
        curvas_data.append(df_plot)
    
    if resultados:
        # Combinar datos de todas las curvas para graficar
        curvas_df = pd.concat(curvas_data, ignore_index=True)
        # Paso 6: Gr√°fico interactivo de curvas acumuladas
        chart = alt.Chart(curvas_df).mark_line().encode(
            x=alt.X("dia:Q", title="D√≠a del a√±o"),
            y=alt.Y("acumulado_norm:Q", title="Fracci√≥n acumulada"),
            color=alt.Color("serie:N", title="Serie (Archivo/A√±o)"),
            tooltip=["serie:N", "dia:Q", alt.Tooltip("acumulado_norm:Q", format=".2f")]
        ).properties(title="Curvas acumuladas normalizadas por a√±o", width=700, height=400)
        st.altair_chart(chart, use_container_width=True)
        
        # Paso 7: Mostrar tabla de resultados
        resultados_df = pd.DataFrame(resultados)
        # Formato: limitar a 3 decimales las columnas num√©ricas para visualizaci√≥n
        resultados_df_display = resultados_df.copy()
        resultados_df_display["AUC_total"] = resultados_df_display["AUC_total"].map(lambda x: f"{x:.2f}")
        resultados_df_display["AUC_dia121"] = resultados_df_display["AUC_dia121"].map(lambda x: f"{x:.2f}")
        resultados_df_display["Prop_121"] = resultados_df_display["Prop_121"].map(lambda x: f"{x:.3f}")
        st.subheader("Resultados por archivo")
        st.dataframe(resultados_df_display, use_container_width=True)
        
        # Paso 8: Bot√≥n de descarga de resultados en CSV
        csv_data = resultados_df.to_csv(index=False).encode('utf-8')
        st.download_button("üì• Descargar tabla CSV", data=csv_data, file_name="resultados_emergencia.csv", mime="text/csv")
    else:
        st.warning("No se generaron resultados. Verifique que los archivos tengan datos v√°lidos.")
else:
    st.info("‚¨Ü Por favor, cargue uno o varios archivos Excel para comenzar el an√°lisis.")
